#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è°ƒè¯•çˆ¬è™«è„šæœ¬

ç”¨äºæµ‹è¯•GitHubçˆ¬è™«çš„åŸºæœ¬åŠŸèƒ½ï¼Œå¸®åŠ©è¯Šæ–­é—®é¢˜ã€‚
"""

import asyncio
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from app.crawler.github_crawler import GitHubTrendingCrawler
from app.config import get_settings
from loguru import logger


async def test_crawler():
    """æµ‹è¯•çˆ¬è™«åŠŸèƒ½"""
    settings = get_settings()
    crawler = GitHubTrendingCrawler()
    
    print("ğŸ”§ çˆ¬è™«é…ç½®ä¿¡æ¯:")
    print(f"   GitHubåŸºç¡€URL: {settings.github_base_url}")
    print(f"   Trending URL: {settings.github_trending_url}")
    print(f"   è¯·æ±‚è¶…æ—¶: {settings.request_timeout}ç§’")
    print(f"   æœ€å¤§é‡è¯•: {settings.max_retries}æ¬¡")
    print(f"   User-Agent: {settings.user_agent[:50]}...")
    print()
    
    # 1. æµ‹è¯•å¥åº·æ£€æŸ¥
    print("1. æµ‹è¯•GitHubè¿æ¥...")
    try:
        health = await crawler.health_check()
        print(f"   å¥åº·çŠ¶æ€: {health}")
        if not health.get('github_accessible', False):
            print("   âŒ GitHubä¸å¯è®¿é—®ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
            return
        else:
            print("   âœ… GitHubè¿æ¥æ­£å¸¸")
    except Exception as e:
        print(f"   âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        return
    
    print()
    
    # 2. æµ‹è¯•è·å–trendingé¡µé¢
    print("2. æµ‹è¯•è·å–trendingé¡µé¢...")
    try:
        url = crawler._build_url(None, "daily")
        print(f"   è¯·æ±‚URL: {url}")
        
        import aiohttp
        async with aiohttp.ClientSession(**crawler.session_config) as session:
            html_content = await crawler._fetch_page(session, url)
            print(f"   é¡µé¢å†…å®¹é•¿åº¦: {len(html_content)}")
            
            # æ£€æŸ¥é¡µé¢å†…å®¹
            if "trending" in html_content.lower():
                print("   âœ… é¡µé¢åŒ…å«trendingå†…å®¹")
            else:
                print("   âŒ é¡µé¢ä¸åŒ…å«trendingå†…å®¹")
                
            # æ£€æŸ¥æ˜¯å¦æœ‰ä»“åº“åˆ—è¡¨
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html_content, 'html.parser')
            repo_articles = soup.find_all('article', class_='Box-row')
            print(f"   æ‰¾åˆ°ä»“åº“æ•°é‡: {len(repo_articles)}")
            
            if len(repo_articles) == 0:
                print("   âŒ æœªæ‰¾åˆ°ä»“åº“åˆ—è¡¨ï¼Œå¯èƒ½é¡µé¢ç»“æ„å·²å˜åŒ–")
                # ä¿å­˜é¡µé¢å†…å®¹ç”¨äºè°ƒè¯•
                with open('debug_page.html', 'w', encoding='utf-8') as f:
                    f.write(html_content)
                print("   ğŸ’¾ é¡µé¢å†…å®¹å·²ä¿å­˜åˆ° debug_page.html")
            else:
                print("   âœ… æ‰¾åˆ°ä»“åº“åˆ—è¡¨")
                
    except Exception as e:
        print(f"   âŒ è·å–é¡µé¢å¤±è´¥: {e}")
        return
    
    print()
    
    # 3. æµ‹è¯•å®Œæ•´çˆ¬å–æµç¨‹
    print("3. æµ‹è¯•å®Œæ•´çˆ¬å–æµç¨‹...")
    try:
        repositories = await crawler.fetch_trending(limit=5)
        print(f"   æˆåŠŸçˆ¬å– {len(repositories)} ä¸ªä»“åº“")
        
        if repositories:
            print("   å‰3ä¸ªä»“åº“:")
            for i, repo in enumerate(repositories[:3]):
                print(f"     {i+1}. {repo.name} - {repo.stars} stars")
            print("   âœ… çˆ¬å–æˆåŠŸ")
        else:
            print("   âŒ æœªçˆ¬å–åˆ°ä»»ä½•ä»“åº“")
            
    except Exception as e:
        print(f"   âŒ çˆ¬å–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    print()
    print("ğŸ‰ è°ƒè¯•å®Œæˆï¼")


if __name__ == "__main__":
    asyncio.run(test_crawler())